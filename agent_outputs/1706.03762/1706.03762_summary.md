# Attention Is All You Need

URL: http://arxiv.org/abs/1706.03762v7

発表年: 2017

著者:
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin

著者の所属機関:
Google Brain

## 要約

本論文は、系列変換モデルにおいて、従来のリカレントニューラルネットワーク（RNN）や畳み込みニューラルネットワーク（CNN）を完全に排除し、アテンションメカニズムのみに依存する新しいアーキテクチャ「Transformer」を提案しています。RNN の逐次計算による並列化の困難さや長距離依存関係学習の課題に対処するため、Transformer はマルチヘッド自己アテンションを活用し、入力と出力間のグローバルな依存関係を効率的に捉えます。これにより、モデルの計算が大幅に並列化され、学習時間が大幅に短縮されました。実験では、WMT 2014 英語-ドイツ語および英語-フランス語翻訳タスクにおいて、Transformer がこれまでの最先端モデル（アンサンブルモデルを含む）を上回る BLEU スコアを達成し、同時に学習コストも大幅に削減できることを示しました。また、英語構成要素構文解析のような他のタスクへの汎化能力も確認されました。

## 使用された手法

- **Transformer アーキテクチャ**: エンコーダー・デコーダー構造を基盤とし、各層が自己アテンション機構と位置ごとの全結合フィードフォワードネットワークで構成されます。リカレント層や畳み込み層は一切使用されません。
- **マルチヘッドアテンション**: クエリ、キー、値を複数の異なる線形射影に変換し、それぞれに対して並列にアテンション関数を実行します。これにより、モデルが異なる表現部分空間からの情報に共同で注意を払うことを可能にし、単一のアテンションヘッドでは捉えきれない多角的な依存関係を学習します。
- **スケール化ドット積アテンション**: クエリとキーのドット積を計算し、$d_k$の平方根でスケーリングした後、ソフトマックス関数を適用してバリューの重みを決定します。これにより、高い次元でのドット積の大きさが極端に大きくなり、ソフトマックス関数が勾配の小さい領域に押し込まれるのを防ぎます。
- **位置ごとのフィードフォワードネットワーク**: 各エンコーダーおよびデコーダー層内に、各位置に個別に適用される全結合フィードフォワードネットワークが含まれます。これは ReLU 活性化関数を挟んだ 2 つの線形変換で構成されます。
- **位置エンコーディング**: モデルにシーケンスの順序に関する情報を注入するため、入力埋め込みに正弦波と余弦波に基づいた位置エンコーディングが追加されます。これにより、リカレントや畳み込みがなくても位置情報を扱うことができます。
- **残差接続と層正規化**: 各サブ層の出力に対して残差接続が適用され、その後に層正規化が行われます。これにより、深いネットワークの学習が安定します。

## 技術の主要なポイント

- **完全なアテンションベース**: RNN や畳み込み層を使用せず、アテンションメカニズムのみで系列変換を実現した初のモデルです。
- **高い並列化性能**: 逐次計算を最小限に抑え、計算を大幅に並列化することで、従来の RNN ベースモデルと比較して学習時間を劇的に短縮します。
- **効率的な長距離依存関係の学習**: 自己アテンションにより、シーケンス内の任意の 2 つの位置間のパス長が一定（$O(1)$）に保たれるため、長距離依存関係の学習が容易になります。
- **マルチヘッドアテンションによる表現力の向上**: 複数のアテンションヘッドが異なる情報部分空間に注意を払うことで、モデルがより豊富で多角的な依存関係を捉えることを可能にします。
- **スケーリングされたドット積アテンションの安定性**: ドット積の大きさをスケーリングすることで、高次元におけるソフトマックス関数の勾配消失問題を軽減し、学習の安定性を向上させます。
- **学習済み位置エンコーディングの代替**: 正弦波ベースの位置エンコーディングは、学習済み位置埋め込みと同等の性能を示し、訓練時に遭遇しなかった長いシーケンスへの外挿可能性を提供します。

## 先行研究との比較

- **RNN/CNN ベースのモデルとの比較**:

  - **計算の並列性**: RNN は本質的に逐次計算を必要とするため並列化が困難であるのに対し、Transformer は完全なアテンションベースであるため、大幅な並列化が可能です。
  - **長距離依存関係の学習**: RNN（パス長 $O(n)$）や畳み込みネットワーク（スタックによりパス長 $O(n/k)$ または $O(\log_k(n))$）は長距離依存関係の学習に課題を抱えることがあります。対照的に、Transformer の自己アテンションは任意の位置間のパス長を一定 ($O(1)$) に保つため、長距離依存関係を効率的に捉えることができます。
  - **計算複雑度**: シーケンス長 $n$ が表現次元 $d$ よりも小さい場合、自己注意層はリカレント層よりも高速です。
  - **性能と効率**: WMT 2014 翻訳タスクにおいて、Transformer は GNMT、ConvS2S、MoE などの既存の最先端モデル（アンサンブルモデルを含む）と比較して、より高い BLEU スコアを、はるかに低いトレーニングコスト（短い学習時間）で達成しました。

- **アテンションメカニズムの利用**: これまでの多くのモデルでは、アテンションメカニズムはリカレントネットワークと組み合わせて使用されていましたが、Transformer は RNN や畳み込みを一切使用せず、完全にアテンションに依存する初の変換モデルです。

- **解釈可能性**: 自己アテンションは、モデルの注意分布を可視化することで、より解釈可能なモデルを提供できる可能性があり、個々のアテンションヘッドが文の統語的および意味的構造に関連する異なるタスクを学習していることが示唆されました。

## 実験方法

- **データセット**:
  - 機械翻訳: WMT 2014 英語-ドイツ語（約 450 万文ペア）、WMT 2014 英語-フランス語（約 3600 万文）。文は Byte-Pair Encoding または WordPiece Encoding で符号化されました。
  - 構文解析: Penn Treebank の Wall Street Journal (WSJ) 部分（約 4 万訓練文）、およびより大規模な高信頼度/BerkeleyParser コーパスを用いた半教師あり設定。
- **バッチ処理**: 各学習バッチには、おおよそのシーケンス長でバッチ処理された約 25000 のソースおよびターゲットトークンが含まれました。
- **ハードウェア**: 8 基の NVIDIA P100 GPU を搭載した 1 台のマシンを使用。
- **学習スケジュール**: ベースモデルは 12 時間（100,000 ステップ）、大規模モデルは 3.5 日（300,000 ステップ）学習されました。
- **オプティマイザ**: Adam オプティマイザ（$\beta_1=0.9$, $\beta_2=0.98$, $\epsilon=10^{-9}$）を使用。学習率は、$warmup\_steps=4000$の間線形に増加し、その後ステップ数の逆平方根に比例して減少するカスタムスケジュールを採用。
- **正則化**:
  - **Residual Dropout**: 各サブ層の出力および埋め込みと位置エンコーディングの合計に適用（$P_{drop}=0.1$ または $0.3$）。
  - **Label Smoothing**: $\epsilon_{ls}=0.1$ の値を使用。
- **推論**: ビームサーチ（ビームサイズ 4、長さペナルティ $\alpha=0.6$）を使用。ベースモデルは最後の 5 つのチェックポイントを平均し、大規模モデルは最後の 20 のチェックポイントを平均。
- **評価指標**: 機械翻訳タスクでは BLEU スコア、構文解析タスクでは F1 スコアで評価。

## 議論

本研究は、Transformer という新しいモデルアーキテクチャを通じて、完全にアテンション機構に基づいたモデルが、機械翻訳タスクにおいて既存の RNN や畳み込みベースの最先端モデルを凌駕し、同時に学習効率を大幅に向上できることを明確に示しました。特に、Transformer の並列計算能力は、従来の逐次的なアプローチに起因する計算ボトルネックを解消し、非常に高速な学習を可能にしました。

モデルのバリエーションに関する実験では、マルチヘッドアテンションにおけるヘッド数や次元、モデルサイズ（層数、次元）が性能に影響を与えること、またドロップアウトが過学習防止に有効であることが確認されました。正弦波ベースの位置エンコーディングは、学習済み埋め込みと同等の性能を示しつつ、より長いシーケンスへの外挿可能性を提供します。

Transformer は機械翻訳だけでなく、英語構成要素構文解析のような他のタスクにも高い汎化能力を持つことを実証しました。これは、特定のドメイン知識やリカレント構造に依存しないアテンションメカニズムの強力さを示唆しています。

今後の研究課題としては、Transformer をテキスト以外の入力および出力モダリティ（画像、音声、ビデオなど）を持つ問題に拡張すること、大規模な入出力を効率的に処理するための局所的または制限的なアテンション機構を調査すること、そして生成プロセスをより非逐次的にすることが挙げられています。また、アテンションヘッドが文の統語的・意味的構造に関連する異なるタスクを学習している可能性が示唆され、アテンションベースモデルの解釈可能性のさらなる探求が期待されます。
