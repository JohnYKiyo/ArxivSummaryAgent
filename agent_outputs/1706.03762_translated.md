# Attention Is All You Need

URL: http://arxiv.org/abs/1706.03762v7

発表年: 2017

著者:
\AND
Ashish Vaswani\thanks{Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.
Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.

---

\section{はじめに}

リカレントニューラルネットワーク、特に長・短期記憶（Long Short-Term Memory, LSTM）\citep{hochreiter1997}およびゲート付きリカレントユニット（Gated Recurrent Unit, GRU）\citep{gruEval14}ニューラルネットワークは、言語モデリングや機械翻訳といった系列モデリングおよび変換問題において、最先端の手法として確固たる地位を確立しています \citep{sutskever14, bahdanau2014neural, cho2014learning}。それ以来、多数の取り組みがリカレント言語モデルとエンコーダー・デコーダーアーキテクチャの限界を押し広げ続けてきました \citep{wu2016google,luong2015effective,jozefowicz2016exploring}。

リカレントモデルは通常、入力および出力系列のシンボル位置に沿って計算を分解します。これらの位置を計算時間のステップに合わせることで、前の隠れ状態 $h_{t-1}$ および位置 $t$ の入力の関数として、隠れ状態の系列 $h_t$ を生成します。この本質的に逐次的な性質は、学習例内での並列化を妨げます。これは、メモリ制約が例間のバッチ処理を制限するため、系列長が長くなるにつれて深刻な問題となります。
最近の研究では、因数分解トリック \citep{Kuchaiev2017Factorization} と条件付き計算 \citep{shazeer2017outrageously} を通じて計算効率が大幅に向上し、後者の場合はモデル性能も改善しました。しかし、逐次計算という根本的な制約は残っています。

アテンションメカニズムは、さまざまなタスクにおける説得力のある系列モデリングおよび変換モデルの不可欠な部分となっており、入力または出力系列における距離に関係なく依存関係をモデル化することを可能にしています \citep{bahdanau2014neural, structuredAttentionNetworks}。しかし、ごく一部の例外 \citep{decomposableAttnModel} を除いて、このようなアテンションメカニズムはリカレントネットワークと組み合わせて使用されています。

本研究では、リカレンスを避け、入力と出力間の大域的な依存関係を捉えるためにアテンションメカニズムのみに完全に依存するモデルアーキテクチャである Transformer を提案します。Transformer は大幅な並列化を可能にし、8 つの P100 GPU でわずか 12 時間の学習で、翻訳品質において新たな最先端の成果を達成できます。
\section{背景}

逐次計算の削減という目標は、Extended Neural GPU \citep{extendedngpu}、ByteNet \citep{NalBytenet2017}、ConvS2S \citep{JonasFaceNet2017} の基礎も形成しています。これらは全て、畳み込みニューラルネットワークを基本的な構成要素として使用し、全ての入力および出力位置に対して隠れ表現を並列に計算します。これらのモデルでは、任意の 2 つの入力または出力位置からの信号を関連付けるのに必要な演算回数が、位置間の距離に応じて増加します。ConvS2S では線形に、ByteNet では対数的に増加します。このため、遠い位置間の依存関係を学習することがより困難になります \citep{hochreiter2001gradient}。Transformer では、これは定数回の演算に削減されますが、アテンション重み付けされた位置の平均化により実効分解能が低下するというコストが伴います。この影響は、セクション\ref{sec:attention}で説明されている Multi-Head Attention によって打ち消されます。

自己アテンション（時にはイントラアテンションとも呼ばれる）は、シーケンスの表現を計算するために、単一シーケンスの異なる位置を関連付けるアテンションメカニズムです。自己アテンションは、読解、要約、テキスト含意、タスクに依存しない文表現の学習を含む、様々なタスクで成功裏に使用されてきました \citep{cheng2016long, decomposableAttnModel, paulus2017deep, lin2017structured}。

エンドツーエンドメモリネットワークは、シーケンスに合わせた繰り返しではなく、リカレントなアテンションメカニズムに基づいており、単純な言語の質問応答や言語モデリングタスクで優れた性能を示すことが示されています \citep{sukhbaatar2015}。

しかし、我々の知る限り、Transformer は、シーケンスに合わせた RNN や畳み込みを使用せずに、その入出力の表現を計算するために完全に自己アテンションに依存する最初の変換モデルです。
以下のセクションでは、Transformer について説明し、自己アテンションの動機付けを行い、\citep{neural_gpu, NalBytenet2017} や \citep{JonasFaceNet2017} といったモデルに対するその利点を議論します。

# モデルアーキテクチャ

\begin{figure}
\centering
\includegraphics[scale=0.6]{Figures/ModalNet-21}
\caption{The Transformer - model architecture.}
\label{fig:model-arch}
\end{figure}

競合力のあるほとんどのニューラルシーケンス変換モデルは、エンコーダー・デコーダー構造を持っています \citep{cho2014learning,bahdanau2014neural,sutskever14}。ここでは、エンコーダーは記号表現の入力シーケンス $(x_1, ..., x_n)$ を連続表現のシーケンス $\mathbf{z} = (z_1, ..., z_n)$ にマッピングします。$\mathbf{z}$ が与えられると、デコーダーは次に、記号の出力シーケンス $(y_1,...,y_m)$ を一度に 1 要素ずつ生成します。各ステップで、モデルは自己回帰的であり \citep{graves2013generating}、次に生成する際に、以前に生成された記号を追加の入力として使用します。

Transformer はこの全体的なアーキテクチャに従っており、エンコーダーとデコーダーの両方にスタックされた自己注意機構と位置ごとの全結合層を使用しています。これらはそれぞれ、図\ref{fig:model-arch}の左半分と右半分に示されています。

## エンコーダーおよびデコーダーのスタック

**エンコーダー:** エンコーダーは $N=6$ 個の同一の層のスタックで構成されています。各層は 2 つのサブ層を持ちます。1 つ目はマルチヘッド自己注意機構であり、2 つ目は単純な位置ごとの全結合フィードフォワードネットワークです。我々は、2 つのサブ層のそれぞれにわたって残差接続 \citep{he2016deep} を採用し、その後に層正規化 \cite{layernorm2016} を適用しています。つまり、各サブ層の出力は $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$ であり、ここで $\mathrm{Sublayer}(x)$ はサブ層自体によって実装される関数です。これらの残差接続を容易にするため、モデル内のすべてのサブ層および埋め込み層は、次元 $\dmodel=512$ の出力を生成します。

**デコーダー:** デコーダーも $N=6$ 個の同一の層のスタックで構成されています。各エンコーダー層における 2 つのサブ層に加えて、デコーダーは 3 番目のサブ層を挿入します。これはエンコーダー出力のスタック上でマルチヘッド注意を実行します。エンコーダーと同様に、各サブ層の周りに残差接続を採用し、その後に層正規化を適用しています。また、デコーダーのスタック内の自己注意サブ層を修正し、ある位置がその後の位置に注意を向けることを防止しています。このマスキングは、出力埋め込みが 1 ポジションずらされているという事実と組み合わせることで、位置 $i$ の予測が $i$ より小さい位置での既知の出力にのみ依存することを保証します。

## 注意機構 \label{sec:attention}

注意関数は、クエリとキー・バリューのセットをある出力にマッピングするものとして記述できます。クエリ、キー、バリュー、および出力はすべてベクトルです。出力はバリューの重み付き和として計算され、各バリューに割り当てられる重みは、クエリと対応するキーの適合度関数によって計算されます。

### スケーリングされたドット積注意 \label{sec:scaled-dot-prod}

我々の特定の注意機構を「スケーリングされたドット積注意」（図\ref{fig:multi-head-att}）と呼びます。入力は次元 $d_k$ のクエリとキー、および次元 $d_v$ のバリューで構成されます。クエリとすべてのキーのドット積を計算し、それぞれを $\sqrt{d_k}$ で割り、ソフトマックス関数を適用してバリューの重みを得ます。

実際には、クエリのセットに対して注意関数を同時に計算し、それらをマトリックス $Q$ にまとめて格納します。キーとバリューも同様にマトリックス $K$ と $V$ にまとめられます。出力のマトリックスは次のように計算されます。

$$
   \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

最も一般的に使用される 2 つの注意関数は、加算型注意 \citep{bahdanau2014neural} とドット積（乗算型）注意です。ドット積注意は、$\frac{1}{\sqrt{d_k}}$ のスケーリング係数を除いて、我々のアルゴリズムと同一です。加算型注意は、単一の隠れ層を持つフィードフォワードネットワークを使用して適合度関数を計算します。理論的な複雑さは両者で類似していますが、ドット積注意は、高度に最適化された行列乗算コードを使用して実装できるため、実際にははるかに高速でメモリ効率も優れています。

$d_k$ の値が小さい場合、2 つのメカニズムは同様のパフォーマンスを示しますが、$d_k$ の値が大きくなると、スケーリングなしのドット積注意よりも加算型注意の方が優れた性能を発揮します \citep{DBLP:journals/corr/BritzGLL17}。我々は、$d_k$ の値が大きい場合、ドット積の大きさが極端に大きくなり、ソフトマックス関数が極めて小さな勾配を持つ領域に押し込まれることが原因ではないかと考えています\footnote{ドット積がなぜ大きくなるかを示すために、$q$ と $k$ の成分が平均 $0$、分散 $1$ の独立した確率変数であると仮定します。すると、それらのドット積 $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$ は、平均 $0$、分散 $d_k$ となります。}。この影響に対抗するために、ドット積を $\frac{1}{\sqrt{d_k}}$ でスケーリングします。

### マルチヘッドアテンション \label{sec:multihead}

_図 1: (左) スケール化ドット積アテンション。(右) マルチヘッドアテンションは、並列に実行される複数のアテンション層から構成される。_
\begin{figure}
\begin{minipage}[t]{0.5\textwidth}
\centering
Scaled Dot-Product Attention \\
\vspace{0.5cm}
\includegraphics[scale=0.6]{Figures/ModalNet-19}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\centering
Multi-Head Attention \\
\vspace{0.1cm}
\includegraphics[scale=0.6]{Figures/ModalNet-20}  
\end{minipage}

\caption{(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.}
\label{fig:multi-head-att}
\end{figure}

`dmodel`-次元のキー、値、クエリを持つ単一のアテンション関数を実行する代わりに、クエリ、キー、値を、それぞれ$d_k$、$d_k$、$d_v$次元へと、学習された異なる線形射影を用いて$h$回線形射影することが有益であることが分かった。これらの射影されたクエリ、キー、値の各バージョンに対して、並列にアテンション関数を実行し、$d_v$次元の出力値を得る。これらは連結され、再度射影されることで、図\ref{fig:multi-head-att}に示すように最終的な値が生成される。

マルチヘッドアテンションは、モデルが異なる位置にある異なる表現部分空間からの情報に共同で注意を払うことを可能にする。単一のアテンションヘッドでは、平均化がこれを妨げる。

$$
    \mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O
$$

$$
    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)
$$

ここで、射影はパラメータ行列$W^Q_i \in \mathbb{R}^{\dmodel \times d_k}$、$W^K_i \in \mathbb{R}^{\dmodel \times d_k}$、$W^V_i \in \mathbb{R}^{\dmodel \times d_v}$および$W^O \in \mathbb{R}^{hd_v \times \dmodel}$である。

本研究では、$h=8$個の並列アテンション層、すなわちヘッドを採用している。これらの各ヘッドに対して、$d_k=d_v=\dmodel/h=64$を使用する。各ヘッドの次元が縮小されているため、全体の計算コストは完全な次元を持つシングルヘッドアテンションと類似している。

### モデルにおけるアテンションの応用

Transformer はマルチヘッドアテンションを 3 つの異なる方法で利用する。

- 「エンコーダ-デコーダアテンション」層では、クエリは前のデコーダ層から、メモリキーと値はエンコーダの出力から供給される。これにより、デコーダのすべての位置が入力シーケンスのすべての位置に注意を払うことができる。これは、\citep{wu2016google, bahdanau2014neural,JonasFaceNet2017}などのシーケンス-トゥ-シーケンスモデルにおける典型的なエンコーダ-デコーダアテンションメカニズムを模倣している。
- エンコーダには自己アテンション層が含まれている。自己アテンション層では、すべてのキー、値、クエリが同じ場所、この場合はエンコーダの前の層の出力から供給される。エンコーダの各位置は、エンコーダの前の層のすべての位置に注意を払うことができる。
- 同様に、デコーダの自己アテンション層は、デコーダの各位置が、その位置まで（その位置を含む）のデコーダのすべての位置に注意を払うことを可能にする。自己回帰性を保持するため、デコーダにおける左方向の情報フローを防ぐ必要がある。これは、スケール化ドット積アテンションの内部で、不正な接続に対応するソフトマックス入力のすべての値をマスキング（$-\infty$に設定）することによって実装される。図\ref{fig:multi-head-att}を参照のこと。

## 位置ごとのフィードフォワードネットワーク \label{sec:ffn}

アテンションサブ層に加えて、我々のエンコーダおよびデコーダの各層には、各位置に個別に同一に適用される全結合フィードフォワードネットワークが含まれている。これは、ReLU 活性化関数を挟んだ 2 つの線形変換から構成される。

$$
   \mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2
$$

線形変換は異なる位置間で同じだが、層ごとに異なるパラメータを使用する。これを説明する別の方法は、カーネルサイズ 1 の 2 つの畳み込みと見なすことである。入力と出力の次元は$\dmodel=512$であり、内側の層は$d_{ff}=2048$の次元を持つ。

## 埋め込みとソフトマックス

他のシーケンス変換モデルと同様に、入力トークンと出力トークンを`dmodel`次元のベクトルに変換するために学習済み埋め込みを使用する。また、デコーダの出力を予測された次のトークン確率に変換するために、通常の学習済み線形変換とソフトマックス関数を使用する。\citep{press2016using}と同様に、我々のモデルでは、2 つの埋め込み層とソフトマックス前の線形変換の間で同じ重み行列を共有している。埋め込み層では、これらの重みに$\sqrt{\dmodel}$を乗算する。

## 位置エンコーディング

我々のモデルにはリカレントも畳み込みも含まれていないため、シーケンスの順序を利用するために、シーケンス内のトークンの相対的または絶対的な位置に関する情報を注入する必要がある。この目的のため、エンコーダとデコーダのスタックの最下部の入力埋め込みに「位置エンコーディング」を追加する。位置エンコーディングは埋め込みと同じ次元`dmodel`を持つため、両者を合計することができる。位置エンコーディングには、学習されたものと固定されたもの、多くの選択肢がある\citep{JonasFaceNet2017}。

本研究では、異なる周波数の正弦関数と余弦関数を使用する。

$$
    PE_{(pos,2i)} = sin(pos / 10000^{2i/\dmodel})
$$

$$
    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\dmodel})
$$

ここで、$pos$は位置、$i$は次元である。つまり、位置エンコーディングの各次元は正弦波に対応する。波長は$2\pi$から$10000 \cdot 2\pi$までの等比数列をなす。この関数を選んだのは、任意に固定されたオフセット$k$に対して、$PE_{pos+k}$が$PE_{pos}$の線形関数として表現できるため、モデルが相対位置によって容易にアテンションを学習できると仮説を立てたからである。

我々はまた、学習済み位置埋め込み\citep{JonasFaceNet2017}を使用する実験も行ったが、両バージョンがほぼ同一の結果を生み出すことが分かった（Table~\ref{tab:variations}の行(E)参照）。我々は、モデルが訓練中に遭遇したよりも長いシーケンス長に外挿できる可能性があるため、正弦波バージョンを選択した。

# なぜ自己注意か

このセクションでは、自己注意層の様々な側面を、シンボル表現の可変長シーケンス$(x_1, ..., x_n)$を同じ長さの別のシーケンス$(z_1, ..., z_n)$にマッピングするためによく用いられるリカレント層や畳み込み層と比較します。ここで、$x_i, z_i \in \mathbb{R}^d$であり、これは典型的なシーケンス変換エンコーダやデコーダにおける隠れ層のようなものです。自己注意を使用する動機として、以下の 3 つの望ましい特性を考慮します。

1 つ目は、層あたりの総計算複雑度です。
2 つ目は、並列化可能な計算量であり、これは必要な逐次操作の最小数で測定されます。

3 つ目は、ネットワークにおける長距離依存関係間のパス長です。長距離依存関係の学習は、多くのシーケンス変換タスクにおける主要な課題です。このような依存関係を学習する能力に影響を与える主要な要因の 1 つは、ネットワーク内で信号が順方向および逆方向に通過しなければならないパスの長さです。入力および出力シーケンスの任意の位置間のこれらのパスが短いほど、長距離依存関係の学習は容易になります\citep{hochreiter2001gradient}。したがって、異なる層タイプで構成されるネットワークにおける、任意の 2 つの入力および出力位置間の最大パス長も比較します。

| 層タイプ            | 層あたりの計算複雑度     | 逐次操作 | 最大パス長     |
| ------------------- | ------------------------ | -------- | -------------- |
| 自己注意            | $O(n^2 \cdot d)$         | $O(1)$   | $O(1)$         |
| リカレント          | $O(n \cdot d^2)$         | $O(n)$   | $O(n)$         |
| 畳み込み            | $O(k \cdot n \cdot d^2)$ | $O(1)$   | $O(\log_k(n))$ |
| 自己注意 (制限付き) | $O(r \cdot n \cdot d)$   | $O(1)$   | $O(n/r)$       |

_表: 異なる層タイプにおける最大パス長、層あたりの複雑度、最小逐次操作数。$n$はシーケンス長、$d$は表現次元、$k$は畳み込みのカーネルサイズ、$r$は制限付き自己注意における近傍のサイズを示す。_

表\ref{tab:op_complexities}に示されているように、自己注意層はすべての位置を一定数の逐次実行される操作で接続するのに対し、リカレント層は$O(n)$の逐次操作を必要とします。
計算複雑度に関して言えば、シーケンス長$n$が表現次元$d$よりも小さい場合、自己注意層はリカレント層よりも高速です。これは、WordPiece\citep{wu2016google}や Byte-Pair\citep{sennrich2015neural}表現など、機械翻訳における最先端モデルが使用する文表現の場合に最もよく当てはまります。
非常に長いシーケンスを伴うタスクの計算性能を向上させるため、自己注意を、対応する出力位置を中心とする入力シーケンス内のサイズ$r$の近傍のみを考慮するように制限することができます。これにより、最大パス長は$O(n/r)$に増加します。このアプローチについては、今後の研究でさらに調査する予定です。

カーネル幅$k < n$を持つ単一の畳み込み層は、入力と出力のすべてのペアを接続するわけではありません。これを行うには、連続カーネルの場合は$O(n/k)$の畳み込み層のスタック、または膨張畳み込み\citep{NalBytenet2017}の場合は$O(\log_k(n))$の畳み込み層のスタックが必要となり、ネットワーク内の任意の 2 つの位置間の最長パスの長さが増加します。
畳み込み層は一般的に、リカレント層よりも$k$倍高価です。ただし、分離可能な畳み込み\citep{xception2016}は、複雑度を$O(k \cdot n \cdot d + n \cdot d^2)$に大幅に減少させます。しかし、$k=n$の場合でも、分離可能な畳み込みの複雑度は、自己注意層とポイントワイズフィードフォワード層の組み合わせ（我々のモデルが採用しているアプローチ）と同等になります。

付帯的な利点として、自己注意はより解釈可能なモデルをもたらす可能性があります。我々はモデルの注意分布を調査し、付録で例を提示し議論しています。個々の注意ヘッドは明らかに異なるタスクを実行することを学習しているだけでなく、多くは文の統語的および意味的構造に関連する振る舞いを示しているように見えます。

# 学習

このセクションでは、我々のモデルの学習方式について説明します。

## 学習データとバッチ処理

我々は、約 450 万の文ペアからなる標準的な WMT 2014 英独データセットで学習を行いました。文は Byte-Pair Encoding\citep{DBLP:journals/corr/BritzGLL17}を使用して符号化され、約 37000 トークンの共有ソース-ターゲット語彙を持ちます。英仏については、3600 万の文からなる大幅に大規模な WMT 2014 英仏データセットを使用し、トークンを 32000 語の WordPiece 語彙\citep{wu2016google}に分割しました。文ペアは、おおよそのシーケンス長によってバッチ処理されました。各学習バッチには、約 25000 のソーストークンと 25000 のターゲットトークンを含む文ペアのセットが含まれていました。

## ハードウェアとスケジュール

我々は、8 基の NVIDIA P100 GPU を搭載した 1 台のマシンでモデルを学習させました。本論文全体で説明されているハイパーパラメータを使用するベースモデルの場合、各学習ステップには約 0.4 秒かかりました。ベースモデルは合計 100,000 ステップまたは 12 時間学習させました。ビッグモデル（表\ref{tab:variations}の最下行に記載）の場合、ステップ時間は 1.0 秒でした。ビッグモデルは 300,000 ステップ（3.5 日）学習させました。

## オプティマイザ

我々は Adam オプティマイザ\citep{kingma2014adam}を使用し、$\beta_1=0.9$, $\beta_2=0.98$, $\epsilon=10^{-9}$としました。学習中は以下の式に従って学習率を変化させました。

$$
lrate = \dmodel^{-0.5} \cdot
  \min({step\_num}^{-0.5},
    {step\_num} \cdot {warmup\_steps}^{-1.5})
$$

これは、$warmup\_steps$学習ステップの間、学習率を線形に増加させ、その後はステップ数の逆平方根に比例して減少させることに対応します。我々は$warmup\_steps=4000$を使用しました。

## 正則化

学習中に 3 種類の正則化手法を用いた。

**Residual Dropout**
ドロップアウト \citep{srivastava2014dropout} を各サブレイヤーの出力に適用した。これは、サブレイヤー入力に追加され、正規化される前に行われる。さらに、エンコーダおよびデコーダスタックの両方において、埋め込みと位置エンコーディングの合計にもドロップアウトを適用した。ベースモデルでは、$P_{drop}=0.1$ のレートを使用した。

**Label Smoothing**
学習中、$\epsilon_{ls}=0.1$ の値を持つラベルスムージング \citep{DBLP:journals/corr/SzegedyVISW15} を用いた。これにより、モデルはより不確実になることを学習するため、パープレキシティは悪化するが、精度と BLEU スコアは向上する。

# 結果

## 機械翻訳

| モデル                                                           | BLEU (EN-DE) | BLEU (EN-FR) | Training Cost (FLOPs) EN-DE | Training Cost (FLOPs) EN-FR |
| ---------------------------------------------------------------- | ------------ | ------------ | --------------------------- | --------------------------- |
| ByteNet \citep{NalBytenet2017}                                   | 23.75        |              |                             |                             |
| Deep-Att + PosUnk \citep{DBLP:journals/corr/ZhouCWLX16}          |              | 39.2         |                             | $1.0\cdot10^{20}$           |
| GNMT + RL \citep{wu2016google}                                   | 24.6         | 39.92        | $2.3\cdot10^{19}$           | $1.4\cdot10^{20}$           |
| ConvS2S \citep{JonasFaceNet2017}                                 | 25.16        | 40.46        | $9.6\cdot10^{18}$           | $1.5\cdot10^{20}$           |
| MoE \citep{shazeer2017outrageously}                              | 26.03        | 40.56        | $2.0\cdot10^{19}$           | $1.2\cdot10^{20}$           |
| ---                                                              | ---          | ---          | ---                         | ---                         |
| Deep-Att + PosUnk Ensemble \citep{DBLP:journals/corr/ZhouCWLX16} |              | 40.4         |                             | $8.0\cdot10^{20}$           |
| GNMT + RL Ensemble \citep{wu2016google}                          | 26.30        | 41.16        | $1.8\cdot10^{20}$           | $1.1\cdot10^{21}$           |
| ConvS2S Ensemble \citep{JonasFaceNet2017}                        | 26.36        | **41.29**    | $7.7\cdot10^{19}$           | $1.2\cdot10^{21}$           |
| ---                                                              | ---          | ---          | ---                         | ---                         |
| Transformer (base model)                                         | 27.3         | 38.1         | **$3.3\cdot10^{18}$**       |                             |
| Transformer (big)                                                | **28.4**     | **41.8**     | $2.3\cdot10^{19}$           |                             |

_表: Transformer は、英語-ドイツ語および英語-フランス語の newstest2014 テストにおいて、従来の最先端モデルよりも優れた BLEU スコアを、はるかに低いトレーニングコストで達成した。_

WMT 2014 英語-ドイツ語翻訳タスクにおいて、大規模な Transformer モデル（表~\ref{tab:wmt-results}の Transformer (big)）は、以前に報告された最良のモデル（アンサンブルモデルを含む）を 2.0 BLEU 以上上回り、28.4 という新しい最先端の BLEU スコアを確立した。このモデルの構成は、表~\ref{tab:variations}の最下行に記載されている。トレーニングには、8 基の P100 GPU で 3.5 日かかった。我々のベースモデルでさえ、これまでに発表されたすべてのモデルおよびアンサンブルモデルを凌駕し、競合するどのモデルと比べてもはるかに低いトレーニングコストで達成された。

WMT 2014 英語-フランス語翻訳タスクでは、我々の大規模モデルが 41.0 の BLEU スコアを達成し、以前に発表されたすべての単一モデルを上回り、従来の最先端モデルのトレーニングコストの 1/4 未満で達成した。英語-フランス語向けに学習された Transformer (big) モデルでは、ドロップアウト率を 0.3 ではなく、$P_{drop}=0.1$ を使用した。

ベースモデルの場合、10 分間隔で保存された最後の 5 つのチェックポイントを平均することによって得られた単一モデルを使用した。大規模モデルの場合、最後の 20 のチェックポイントを平均した。ビームサイズ 4、長さペナルティ $\alpha=0.6$ \citep{wu2016google} のビームサーチを使用した。これらのハイパーパラメータは、開発セットでの実験後に選択された。推論中の最大出力長は、入力長 + 50 に設定したが、可能な場合は早期に終了させた \citep{wu2016google}。

表~\ref{tab:wmt-results}は、我々の結果をまとめ、我々の翻訳品質とトレーニングコストを文献の他のモデルアーキテクチャと比較している。モデルのトレーニングに使用された浮動小数点演算の数は、トレーニング時間、使用された GPU の数、および各 GPU の持続的な単精度浮動小数点演算能力の推定値を乗算して推定した。\footnote{K80、K40、M40、P100 については、それぞれ 2.8、3.7、6.0、9.5 TFLOPS の値を使用した。}

## モデルのバリエーション

|      | $N$ |     $\dmodel$     |                    $\dff$                    | $h$ | $d_k$ | $d_v$ | $P_{drop}$ | $\epsilon_{ls}$ | train steps | PPL (dev) | BLEU (dev) | params $\times10^6$ |
| :--- | :-: | :---------------: | :------------------------------------------: | :-: | :---: | :---: | :--------: | :-------------: | :---------: | :-------: | :--------: | :-----------------: |
| base |  6  |        512        |                     2048                     |  8  |  64   |  64   |    0.1     |       0.1       |    100K     |   4.92    |    25.8    |         65          |
|      |     |                   |                                              |     |       |       |            |                 |             |           |            |                     |
| (A)  |     |                   |                                              |  1  |  512  |  512  |            |                 |             |   5.29    |    24.9    |                     |
|      |     |                   |                                              |  4  |  128  |  128  |            |                 |             |   5.00    |    25.5    |                     |
|      |     |                   |                                              | 16  |  32   |  32   |            |                 |             |   4.91    |    25.8    |                     |
|      |     |                   |                                              | 32  |  16   |  16   |            |                 |             |   5.01    |    25.4    |                     |
|      |     |                   |                                              |     |       |       |            |                 |             |           |            |                     |
| (B)  |     |                   |                                              |     |  16   |       |            |                 |             |   5.16    |    25.1    |         58          |
|      |     |                   |                                              |     |  32   |       |            |                 |             |   5.01    |    25.4    |         60          |
|      |     |                   |                                              |     |       |       |            |                 |             |           |            |                     |
| (C)  |  2  |                   |                                              |     |       |       |            |                 |             |   6.11    |    23.7    |         36          |
|      |  4  |                   |                                              |     |       |       |            |                 |             |   5.19    |    25.3    |         50          |
|      |  8  |                   |                                              |     |       |       |            |                 |             |   4.88    |    25.5    |         80          |
|      |     |        256        |                                              |     |  32   |  32   |            |                 |             |   5.75    |    24.5    |         28          |
|      |     |       1024        |                                              |     |  128  |  128  |            |                 |             |   4.66    |    26.0    |         168         |
|      |     |                   |                     1024                     |     |       |       |            |                 |             |   5.12    |    25.4    |         53          |
|      |     |                   |                     4096                     |     |       |       |            |                 |             |   4.75    |    26.2    |         90          |
|      |     |                   |                                              |     |       |       |            |                 |             |           |            |                     |
| (D)  |     |                   |                                              |     |       |       |    0.0     |                 |             |   5.77    |    24.6    |                     |
|      |     |                   |                                              |     |       |       |    0.2     |                 |             |   4.95    |    25.5    |                     |
|      |     |                   |                                              |     |       |       |            |       0.0       |             |   4.67    |    25.3    |                     |
|      |     |                   |                                              |     |       |       |            |       0.2       |             |   5.47    |    25.7    |                     |
|      |     |                   |                                              |     |       |       |            |                 |             |           |            |                     |
| (E)  |     | \multicolumn{7}{c | }{positional embedding instead of sinusoids} |     | 4.92  | 25.7  |            |
|      |     |                   |                                              |     |       |       |            |                 |             |           |            |                     |
| big  |  6  |       1024        |                     4096                     | 16  |       |       |    0.3     |                 |    300K     | **4.33**  |  **26.4**  |         213         |

_表: Transformer アーキテクチャのバリエーション。リストされていない値はベースモデルと同一。全ての評価指標は、英語からドイツ語への翻訳開発セット (newstest2013) におけるもの。示されているパープレキシティは、我々のバイトペアエンコーディングに基づくワードピース単位であり、単語単位のパープレキシティと比較すべきではない。_

Transformer の異なるコンポーネントの重要性を評価するため、我々はベースモデルを様々な方法で変更し、開発セット newstest2013 における英語からドイツ語への翻訳性能の変化を測定した。前節で述べたビームサーチを使用したが、チェックポイント平均化は行わなかった。これらの結果を表~\ref{tab:variations}に示す。

表~\ref{tab:variations}の行(A)では、セクション~\ref{sec:multihead}で述べたように、計算量を一定に保ちながら、アテンションヘッドの数とアテンションのキー・バリュー次元を変更している。シングルヘッドアテンションは最良の設定よりも BLEU スコアが 0.9 低下したが、ヘッドが多すぎても品質は低下する。

表~\ref{tab:variations}の行(B)では、アテンションキーサイズ$d_k$を減らすとモデルの品質が損なわれることがわかった。これは、互換性を決定することが容易ではなく、ドット積よりも洗練された互換性関数が有益である可能性を示唆している。さらに、行(C)と(D)では、予想通り、より大きなモデルの方が優れており、ドロップアウトが過学習を防ぐのに非常に役立つことが観察された。行(E)では、正弦波位置エンコーディングを学習済み位置埋め込み\citep{JonasFaceNet2017}に置き換えたが、ベースモデルとほぼ同じ結果が得られた。

## 英語構成要素構文解析

| **Parser**                                             | **Training**             | **WSJ 23 F1** |
| :----------------------------------------------------- | :----------------------- | :-----------: |
| Vinyals \& Kaiser el al. (2014) \cite{KVparse15}       | WSJ only, discriminative |     88.3      |
| Petrov et al. (2006) \cite{petrov-EtAl:2006:ACL}       | WSJ only, discriminative |     90.4      |
| Zhu et al. (2013) \cite{zhu-EtAl:2013:ACL}             | WSJ only, discriminative |     90.4      |
| Dyer et al. (2016) \cite{dyer-rnng:16}                 | WSJ only, discriminative |     91.7      |
|                                                        |                          |               |
| Transformer (4 layers)                                 | WSJ only, discriminative |     91.3      |
|                                                        |                          |               |
| Zhu et al. (2013) \cite{zhu-EtAl:2013:ACL}             | semi-supervised          |     91.3      |
| Huang \& Harper (2009) \cite{huang-harper:2009:EMNLP}  | semi-supervised          |     91.3      |
| McClosky et al. (2006) \cite{mcclosky-etAl:2006:NAACL} | semi-supervised          |     92.1      |
| Vinyals \& Kaiser el al. (2014) \cite{KVparse15}       | semi-supervised          |     92.1      |
|                                                        |                          |               |
| Transformer (4 layers)                                 | semi-supervised          |     92.7      |
|                                                        |                          |               |
| Luong et al. (2015) \cite{multiseq2seq}                | multi-task               |     93.0      |
| Dyer et al. (2016) \cite{dyer-rnng:16}                 | generative               |     93.3      |

_表: Transformer は英語の構成要素構文解析によく汎化する（結果は WSJ のセクション 23 に基づく）_

Transformer が他のタスクに汎化できるかを評価するため、我々は英語の構成要素構文解析に関する実験を行った。このタスクは特定の課題を提示する：出力は強い構造的制約を受け、入力よりもかなり長い。さらに、RNN シーケンス-トゥ-シーケンスモデルは、小規模データ体制では最先端の結果を達成できていない\cite{KVparse15}。

我々は、Penn Treebank \citep{marcus1993building}のウォール・ストリート・ジャーナル (WSJ) 部分（約 4 万の訓練文）に対して、$d_{model} = 1024$の 4 層 Transformer を訓練した。また、約 1700 万文のより大きな高信頼度および BerkleyParser コーパス\citep{KVparse15}を用いて、半教師あり設定でも訓練した。WSJ のみの設定では 16K トークンの語彙を、半教師あり設定では 32K トークンの語彙を使用した。

我々は、セクション 22 の開発セットにおいて、ドロップアウト（アテンションと残差の両方、セクション~\ref{sec:reg}）、学習率、ビームサイズを選択するために少数の実験を行っただけで、他のすべてのパラメータは英語からドイツ語へのベース翻訳モデルから変更されていない。推論時には、最大出力長を入力長 + $300$に増やした。WSJ のみの設定と半教師あり設定の両方で、ビームサイズ$21$、$\alpha=0.3$を使用した。

表~\ref{tab:parsing-results}に示す我々の結果は、タスク固有のチューニングを行っていないにもかかわらず、本モデルが驚くほど良好な性能を示し、Recurrent Neural Network Grammar \cite{dyer-rnng:16}を除くこれまでに報告されたすべてのモデルよりも優れた結果をもたらしていることを示している。

RNN シーケンス-トゥ-シーケンスモデル\citep{KVparse15}とは対照的に、Transformer は、WSJ 訓練セットの 4 万文のみで訓練した場合でも、BerkeleyParser \cite{petrov-EtAl:2006:ACL}を上回る性能を示している。
\section{結論}
本研究では、Transformer を発表しました。これは、エンコーダー・デコーダーアーキテクチャで最も一般的に使用されている再帰層をマルチヘッド自己注意に置き換え、完全に注意機構に基づいた最初の系列変換モデルです。

翻訳タスクにおいて、Transformer は再帰層や畳み込み層に基づくアーキテクチャよりも大幅に高速に学習できます。WMT 2014 英独翻訳タスクと WMT 2014 英仏翻訳タスクの両方で、私たちは新たな SOTA を達成しました。前者のタスクでは、私たちの最高のモデルは、これまでに報告されたすべてのアンサンブルモデルさえも上回りました。

私たちは注意ベースのモデルの将来に期待しており、それらを他のタスクにも適用する予定です。Transformer をテキスト以外の入力および出力モダリティを伴う問題に拡張し、画像、音声、ビデオなどの大規模な入力および出力を効率的に処理するための局所的・制限的な注意機構を調査する予定です。生成をより非シーケンシャルにすることも、私たちの研究目標の一つです。

モデルの学習と評価に使用したコードは、\url{https://github.com/tensorflow/tensor2tensor} で公開されています。

\paragraph{謝辞}
Nal Kalchbrenner 氏と Stephan Gouws 氏の有益なコメント、修正、インスピレーションに感謝いたします。

\begin{thebibliography}{10}

\bibitem{layernorm2016}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
translate.
\newblock {\em CoRR}, abs/1409.0473, 2014.

\bibitem{DBLP:journals/corr/BritzGLL17}
Denny Britz, Anna Goldie, Minh{-}Thang Luong, and Quoc~V. Le.
\newblock Massive exploration of neural machine translation architectures.
\newblock {\em CoRR}, abs/1703.03906, 2017.

\bibitem{cheng2016long}
Jianpeng Cheng, Li~Dong, and Mirella Lapata.
\newblock Long short-term memory-networks for machine reading.
\newblock {\em arXiv preprint arXiv:1601.06733}, 2016.

\bibitem{cho2014learning}
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for
statistical machine translation.
\newblock {\em CoRR}, abs/1406.1078, 2014.

\bibitem{xception2016}
Francois Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock {\em arXiv preprint arXiv:1610.02357}, 2016.

\bibitem{gruEval14}
Junyoung Chung, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Kyunghyun Cho, and Yoshua
Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
modeling.
\newblock {\em CoRR}, abs/1412.3555, 2014.

\bibitem{dyer-rnng:16}
Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah~A. Smith.
\newblock Recurrent neural network grammars.
\newblock In {\em Proc. of NAACL}, 2016.

\bibitem{JonasFaceNet2017}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N. Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock {\em arXiv preprint arXiv:1705.03122v2}, 2017.

\bibitem{graves2013generating}
Alex Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1308.0850}, 2013.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition}, pages 770--778, 2016.

\bibitem{hochreiter2001gradient}
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J{\"u}rgen Schmidhuber.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
dependencies, 2001.

\bibitem{hochreiter1997}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{huang-harper:2009:EMNLP}
Zhongqiang Huang and Mary Harper.
\newblock Self-training {PCFG} grammars with latent annotations across
languages.
\newblock In {\em Proceedings of the 2009 Conference on Empirical Methods in
Natural Language Processing}, pages 832--841. ACL, August 2009.

\bibitem{jozefowicz2016exploring}
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
\newblock Exploring the limits of language modeling.
\newblock {\em arXiv preprint arXiv:1602.02410}, 2016.

\bibitem{extendedngpu}
{\L}ukasz Kaiser and Samy Bengio.
\newblock Can active memory replace attention?
\newblock In {\em Advances in Neural Information Processing Systems, ({NIPS})}, 2016.

\bibitem{neural_gpu}
{\L}ukasz Kaiser and Ilya Sutskever.
\newblock Neural {GPU}s learn algorithms.
\newblock In {\em International Conference on Learning Representations
({ICLR})}, 2016.

\bibitem{NalBytenet2017}
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van~den Oord, Alex
Graves, and Koray Kavukcuoglu.
\newblock Neural machine translation in linear time.
\newblock {\em arXiv preprint arXiv:1610.10099v2}, 2017.

\bibitem{structuredAttentionNetworks}
Yoon Kim, Carl Denton, Luong Hoang, and Alexander~M. Rush.
\newblock Structured attention networks.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em ICLR}, 2015.

\bibitem{Kuchaiev2017Factorization}
Oleksii Kuchaiev and Boris Ginsburg.
\newblock Factorization tricks for {LSTM} networks.
\newblock {\em arXiv preprint arXiv:1703.10722}, 2017.

\bibitem{lin2017structured}
Zhouhan Lin, Minwei Feng, Cicero Nogueira~dos Santos, Mo~Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio.
\newblock A structured self-attentive sentence embedding.
\newblock {\em arXiv preprint arXiv:1703.03130}, 2017.

\bibitem{multiseq2seq}
Minh-Thang Luong, Quoc~V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.
\newblock Multi-task sequence to sequence learning.
\newblock {\em arXiv preprint arXiv:1511.06114}, 2015.

\bibitem{luong2015effective}
Minh-Thang Luong, Hieu Pham, and Christopher~D Manning.
\newblock Effective approaches to attention-based neural machine translation.
\newblock {\em arXiv preprint arXiv:1508.04025}, 2015.

\bibitem{marcus1993building}
Mitchell~P Marcus, Mary~Ann Marcinkiewicz, and Beatrice Santorini.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock {\em Computational linguistics}, 19(2):313--330, 1993.

\bibitem{mcclosky-etAl:2006:NAACL}
David McClosky, Eugene Charniak, and Mark Johnson.
\newblock Effective self-training for parsing.
\newblock In {\em Proceedings of the Human Language Technology Conference of
the NAACL, Main Conference}, pages 152--159. ACL, June 2006.

\bibitem{decomposableAttnModel}
Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model.
\newblock In {\em Empirical Methods in Natural Language Processing}, 2016.

\bibitem{paulus2017deep}
Romain Paulus, Caiming Xiong, and Richard Socher.
\newblock A deep reinforced model for abstractive summarization.
\newblock {\em arXiv preprint arXiv:1705.04304}, 2017.

\bibitem{petrov-EtAl:2006:ACL}
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
\newblock Learning accurate, compact, and interpretable tree annotation.
\newblock In {\em Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting of the ACL}, pages
433--440. ACL, July 2006.

\bibitem{press2016using}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock {\em arXiv preprint arXiv:1608.05859}, 2016.

\bibitem{sennrich2015neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock {\em arXiv preprint arXiv:1508.07909}, 2015.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer.
\newblock {\em arXiv preprint arXiv:1701.06538}, 2017.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and
Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(1):1929--1958, 2014.

\bibitem{sukhbaatar2015}
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
\newblock End-to-end memory networks.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
editors, {\em Advances in Neural Information Processing Systems 28}, pages
2440--2448. Curran Associates, Inc., 2015.

\bibitem{sutskever14}
Ilya Sutskever, Oriol Vinyals, and Quoc~VV Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
3104--3112, 2014.

\bibitem{DBLP:journals/corr/SzegedyVISW15}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and
Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock {\em CoRR}, abs/1512.00567, 2015.

\bibitem{KVparse15}
{Vinyals {\&} Kaiser}, Koo, Petrov, Sutskever, and Hinton.
\newblock Grammar as a foreign language.
\newblock In {\em Advances in Neural Information Processing Systems}, 2015.

\bibitem{wu2016google}
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et~al.
\newblock Google's neural machine translation system: Bridging the gap between
human and machine translation.
\newblock {\em arXiv preprint arXiv:1609.08144}, 2016.

\bibitem{DBLP:journals/corr/ZhouCWLX16}
Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.
\newblock Deep recurrent models with fast-forward connections for neural
machine translation.
\newblock {\em CoRR}, abs/1606.04199, 2016.

\bibitem{zhu-EtAl:2013:ACL}
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.
\newblock Fast and accurate shift-reduce constituent parsing.
\newblock In {\em Proceedings of the 51st Annual Meeting of the ACL (Volume 1:
Long Papers)}, pages 434--443. ACL, August 2013.

\end{thebibliography}
\pagebreak
\section*{注意機構の可視化}\label{sec:viz-att}
*図 1: エンコーダーの自己注意の層 5（全 6 層中）における、長距離依存関係を追跡する注意機構の例。「making」という動詞の離れた依存関係の多くが「making...more difficult」というフレーズを完成させている。ここでは「making」という単語の注意のみを示す。異なる色は異なるヘッドを表す。カラー表示を推奨。\*
{\includegraphics[width=\textwidth, trim=0 0 0 36, clip]{./vis/making_more_difficult5_new.pdf}}

_図 2: 係り受け解消に関与していると思われる、同じく層 5（全 6 層中）の 2 つの注意ヘッド。上: ヘッド 5 の完全な注意。下: 注意ヘッド 5 と 6 における「its」という単語からの注意のみを分離したもの。この単語に対する注意が非常にシャープであることに注目。_
{\includegraphics[width=\textwidth, trim=0 0 0 45, clip]{./vis/anaphora_resolution_new.pdf}}
{\includegraphics[width=\textwidth, trim=0 0 0 37, clip]{./vis/anaphora_resolution2_new.pdf}}

_図 3: 注意ヘッドの多くは、文の構造に関連していると思われる挙動を示す。上記に、エンコーダーの自己注意の層 5（全 6 層中）から得られた 2 つの異なるヘッドの例を 2 つ示す。各ヘッドが異なるタスクを学習していることが明確である。_
{\includegraphics[width=\textwidth, trim=0 0 0 36, clip]{./vis/attending_to_head_new.pdf}}
{\includegraphics[width=\textwidth, trim=0 0 0 36, clip]{./vis/attending_to_head2_new.pdf}}

\section\*{2 つのフィードフォワード層 = パラメータ上の注意機構}\label{sec:parameter_attention}

注意層に加えて、私たちのモデルには位置ごとのフィードフォワードネットワーク（セクション\ref{sec:ffn}）が含まれており、これらは ReLU 活性化関数を挟んだ 2 つの線形変換から構成されています。実際、これらのネットワークも一種の注意機構と見なすことができます。このようなネットワークの式と単純なドット積注意層の式（バイアスとスケーリング因子は省略）を比較してみましょう。

$$
FFN(x, W_1, W_2) = ReLU(xW_1)W_2 \\
A(q, K, V) = Softmax(qK^T)V
$$

これらの式の類似性に基づき、2 層フィードフォワードネットワークは一種の注意機構と見なすことができ、そこではキーと値が学習可能なパラメータ行列$W_1$と$W_2$の行であり、適合度関数では Softmax の代わりに ReLU を使用します。

この類似性を踏まえ、私たちは位置ごとのフィードフォワードネットワークを、モデルの他の場所で使用している注意層と同様の注意層に置き換える実験を行いました。パラメータ上のマルチヘッド注意サブ層は、\ref{sec:multihead}で説明されているマルチヘッド注意と同一ですが、各注意ヘッドへの「キー」と「値」の入力が、前層の線形射影ではなく、学習可能なモデルパラメータである点が異なります。これらのパラメータは、活性化関数により類似するように$\sqrt{d_{model}}$倍にスケーリングされます。

最初の実験では、各位置ごとのフィードフォワードネットワークを、$h_p=8$ヘッド、キー次元$d_{pk}=64$、値次元$d_{pv}=64$を持つパラメータ上のマルチヘッド注意サブ層に置き換え、各注意ヘッドに$n_p=1536$のキーと値のペアを使用しました。このサブ層は、クエリ射影と出力射影のパラメータを含め、合計$2097152$個のパラメータを持っています。これは、置き換えた位置ごとのフィードフォワードネットワークのパラメータ数と一致します。理論上の計算量は同じですが、実際には、注意バージョンの方がステップ時間が約 30%長くなりました。

2 番目の実験では、$h_p=8$ヘッド、各注意ヘッドに$n_p=512$のキーと値のペアを使用し、これもベースモデルの総パラメータ数と一致させました。

最初の実験の結果はベースモデルよりもわずかに悪く、2 番目の実験の結果はわずかに良くなりました。詳細は表\ref{tab:parameter_attention}を参照してください。

_表: 位置ごとのフィードフォワードネットワークをパラメータ上のマルチヘッド注意に置き換えることで、ベースモデルと同様の結果が得られる。すべての指標は英独翻訳開発セット、newstest2013 におけるもの。_
\label{tab:parameter*attention}
| | $\dmodel$ | $\dff$ | $h_p$ | $d*{pk}$ | $d_{pv}$ | $n_p$ | PPL (dev) | BLEU (dev) | params $\times10^6$ | training time |
|---|---|---|---|---|---|---|---|---|---|---|
| base | 512 | 2048 | | | | | 4.92 | 25.8 | 65 | 12 hours |
| AOP$_1$ | 512 | | 8 | 64 | 64 | 1536 | 4.92 | 25.5 | 65 | 16 hours |
| AOP$_2$ | 512 | | 16 | 64 | 64 | 512 | **4.86** | **25.9** | 65 | 16 hours |

\section\*{ドット積注意におけるスケーリング因子の正当性}

セクション\ref{sec:scaled-dot-prod}では、ドット積を$\sqrt{d_k}$でスケーリングするスケール付きドット積注意を紹介しました。このセクションでは、このスケーリング因子の大まかな正当化を行います。$q$と$k$が、平均$0$、分散$1$の独立な確率変数である成分を持つ$d_k$次元ベクトルであると仮定すると、それらのドット積$q \cdot k = \sum_{i=1}^{d_k} u_iv_i$は、平均$0$、分散$d_k$になります。これらの値が分散$1$を持つことが望ましいため、$\sqrt{d_k}$で除算します。
