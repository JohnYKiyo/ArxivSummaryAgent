# Proximal Policy Optimization Algorithms

URL: http://arxiv.org/abs/1707.06347v2

発表年: 2017

著者:
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov

著者の所属機関:

OpenAI

## 要約

本論文は、強化学習のための新しい方策勾配法ファミリーである近傍方策最適化（PPO）アルゴリズムを提案しています。PPOは、環境との相互作用によるデータサンプリングと、確率的勾配上昇を用いた「代替」目的関数の最適化を交互に行う点が特徴です。標準的な方策勾配法がデータサンプルごとに1回の勾配更新を行うのに対し、PPOはミニバッチ更新の複数エポックを可能にする新しい目的関数を導入しています。この新しい手法は、信頼領域方策最適化（TRPO）の利点の一部を備えながらも、実装ははるかに単純で、より一般的であり、経験的に優れたサンプル効率を示します。シミュレートされたロボットの運動やAtariゲームのプレイを含む一連のベンチマークタスクでPPOをテストした結果、他のオンライン方策勾配法を上回り、サンプル効率、単純さ、および実行時間の間の好ましいバランスを取ることが示されました。

## 使用された手法

- **近傍方策最適化 (PPO) アルゴリズム**: 強化学習における方策勾配法の新しいファミリー。
- **クリップされた代替目的関数 ($L^{CLIP}$)**: 方策更新のサイズを制御するために、確率比（新しい方策と古い方策の比率）を特定の範囲（$[1 - \epsilon, 1 + \epsilon]$）にクリッピングする新しい目的関数。これにより、過度に大きな方策更新を抑制し、方策パフォーマンスの悲観的な推定（下限）を最大化します。
- **適応的KLペナルティ係数を持つ代替目的関数 ($L^{KLPEN}$)**: KLダイバージェンスをペナルティ項として含み、各更新でKLダイバージェンスが目標値に近づくようにペナルティ係数を適応的に調整する手法。クリップされた目的関数と比較してパフォーマンスは劣ると報告されていますが、重要なベースラインとして検討されました。
- **汎化アドバンテージ推定 (Generalized Advantage Estimation, GAE)**: 価値関数の推定を利用して、分散を低減したアドバンテージ関数を計算する手法。
- **複合目的関数**: 方策の代替目的関数、価値関数の二乗誤差損失、および探索を促すためのエントロピーボーナスを組み合わせた総合的な損失関数 ($L^{CLIP+VF+S}$)。
- **確率的勾配降下法 (SGD) とAdamオプティマイザ**: 収集したデータに対して複数エポックのミニバッチ更新を実行するために使用されます。

## 技術の主要なポイント

- **安全な方策更新の実現**: 確率比をクリッピングすることで、方策の更新幅が大きくなりすぎることを防ぎ、安定した学習を可能にします。これにより、方策のパフォーマンスが急激に悪化するリスクを低減します。
- **高いサンプル効率**: 収集したデータを複数エポックにわたるミニバッチSGDで再利用することで、バニラ方策勾配法よりもはるかに効率的に学習を進めることができます。これにより、必要な環境との相互作用回数を削減します。
- **実装の単純さ**: TRPOのような複雑な二次最適化手法（共役勾配法など）を必要とせず、標準的な一次最適化ツール（SGD/Adam）のみで実装可能です。これにより、既存の方策勾配コードベースへの導入が容易になります。
- **幅広い適用性**: 方策と価値関数の間のパラメータ共有や、ドロップアウトなどのニューラルネットワークアーキテクチャの変更にも対応可能であり、様々な強化学習設定に適用できます。
- **ロバストなパフォーマンス**: ハイパーパラメータ調整が限定的でも、多様なタスクで信頼性の高いパフォーマンスを発揮し、多くの場合で既存のSOTA手法を上回るか、同等の結果をよりシンプルに達成します。

## 先行研究との比較

- **信頼領域方策最適化 (TRPO)**: PPOはTRPOのデータ効率と信頼性の高いパフォーマンスを維持しつつ、実装の複雑さを大幅に軽減します。TRPOは二次近似と共役勾配法を必要とし、ドロップアウトやパラメータ共有を含むアーキテクチャとの互換性が低いですが、PPOは一次最適化のみを使用し、より一般的な設定に対応します。PPOは信頼領域制約の代わりに、確率比のクリッピングやKLペナルティを用いて更新のサイズを制御します。
- **バニラ方策勾配法**: バニラ方策勾配法と比較して、PPOはデータ効率とロバスト性が大幅に向上しています。PPOは収集したデータを複数回利用し、クリッピングやペナルティによって過度に大きな方策更新を防ぎますが、バニラ方策勾配法はこれらの機構を欠いています。
- **A2C (Asynchronous Advantage Actor-Critic)**: Atariベンチマークにおいて、PPOはA2Cよりもサンプル効率の面で大幅に優れたパフォーマンスを示します。
- **ACER (Actor-Critic with Experience Replay)**: Atariベンチマークにおいて、PPOはACERとほぼ同等のパフォーマンスを示しますが、そのアルゴリズムはACERよりもはるかに単純です。

## 実験方法

- **連続制御タスク (MuJoCo)**: OpenAI Gymに実装された7つのMuJoCoシミュレートされたロボティクス制御タスク（HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, Walker2d）を使用。各タスクで100万タイムステップの学習を行い、最後の100エピソードの平均総報酬を正規化して評価しました。方策は2つの隠れ層と64ユニットを持つ全結合MLPで表現されました。
- **高次元連続制御タスク (Roboschool Humanoid)**: 3Dヒューマノイドが走行、操縦、そしてキューブによって叩かれながら立ち上がる必要があるRoboschool環境のタスクでPPOのパフォーマンスを評価しました。
- **Atariゲーム**: Arcade Learning Environmentの49ゲームでPPOのパフォーマンスを評価し、調整済みのA2CおよびACER実装と比較しました。Mnih et al. (2016) と同じ方策ネットワークアーキテクチャを使用しました。評価指標として、全学習期間と学習の最後の100エピソードにおけるエピソードごとの平均報酬を検討し、3つの試行で平均化しました。
- **目的関数の比較**: クリップなし/ペナルティなし、異なる$\epsilon$値のクリッピング、異なる$d_{\text{targ}}$値の適応的KLペナルティ、異なる$\beta$値の固定KLペナルティなど、様々なPPOのバリアントを比較しました。
- **最適化設定**: すべての実験でAdamオプティマイザを使用したミニバッチSGDが用いられ、ハイパーパラメータ（Horizon (T)、Adamステップサイズ、エポック数、ミニバッチサイズ、割引率($\gamma$)、GAEパラメータ($\lambda$)、クリッピングパラメータ($\epsilon$)、価値関数係数($c_1$)、エントロピー係数($c_2$)）が各環境タイプに応じて調整されました。

## 議論

PPOアルゴリズムは、信頼領域手法の安定性と信頼性を保ちつつ、その実装がはるかに単純であるという大きな利点を持っています。この特性は、バニラ方策勾配の実装に対してわずかなコード変更で済み、既存の深層強化学習システムへの統合を容易にします。また、方策と価値関数のための結合アーキテクチャなど、より一般的なニューラルネットワーク設定にも適用できる汎用性もPPOの強みです。

実験結果は、連続制御タスクとAtariゲームの両方でPPOが既存のオンライン方策勾配法と比較して優れた、または競争力のあるパフォーマンスを示したことを明確に示しています。特に、クリップされた代替目的関数が最も効果的であることが実証され、データ効率と実装の単純さのバランスが優れていることが確認されました。

PPOの成功は、ロバストでデータ効率が高く、かつ簡単に実装できる強化学習アルゴリズムへの需要に応えるものです。今後の研究では、PPOのさらなる理論的理解、より複雑な環境やタスクへの適用、他の強化学習技術（例：オフライン学習、モデルベース学習）との組み合わせによるパフォーマンス向上が考えられます。