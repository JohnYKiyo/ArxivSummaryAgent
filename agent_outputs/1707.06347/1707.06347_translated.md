# Proximal Policy Optimization Algorithms

URL: http://arxiv.org/abs/1707.06347v2

発表年: 2017

著者:
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov

---

# 近傍方策最適化アルゴリズム

ジョン・シュルマン、フィリップ・ウォルスキー、プラフルラ・ダリワル、アレック・ラドフォード、オレグ・クリモフ
OpenAI
{joschu, filip, prafulla, alec, oleg}@openai.com

## 要旨

本稿では、強化学習のための新しい方策勾配法ファミリーを提案する。この手法は、環境との相互作用によるデータサンプリングと、確率的勾配上昇を用いた「代替」目的関数の最適化を交互に行う。標準的な方策勾配法がデータサンプルごとに1回の勾配更新を行うのに対し、我々はミニバッチ更新の複数エポックを可能にする新しい目的関数を提案する。近傍方策最適化（PPO）と呼ぶ新しい手法は、信頼領域方策最適化（TRPO）の利点の一部を備えているが、実装ははるかに単純で、より一般的であり、より良いサンプル効率（経験的に）を持つ。我々の実験では、シミュレートされたロボットの運動やAtariゲームのプレイを含む、一連のベンチマークタスクでPPOをテストし、PPOが他のオンライン方策勾配法を上回り、全体としてサンプル効率、単純さ、および実行時間の間の好ましいバランスを取ることを示す。

# 1 導入

近年、ニューラルネットワーク関数近似器を用いた強化学習のために、いくつかの異なるアプローチが提案されてきた。主要な候補には、深層Q学習 [Mni+15]、「バニラ」方策勾配法 [Mni+16]、および信頼領域/自然方策勾配法 [Sch+15b] がある。しかし、スケーラブル（大規模モデルおよび並列実装に対応可能）、データ効率的、かつロバスト（すなわち、ハイパーパラメータ調整なしに様々な問題で成功する）な手法の開発には改善の余地がある。Q学習（関数近似あり）は多くの単純な問題¹で失敗し、理解が進んでいない。バニラ方策勾配法はデータ効率とロバスト性が低く、信頼領域方策最適化（TRPO）は比較的複雑で、ノイズ（ドロップアウトなど）やパラメータ共有（方策と価値関数の間、または補助タスクとの間）を含むアーキテクチャと互換性がない。

本稿では、TRPOのデータ効率と信頼性の高いパフォーマンスを達成しつつ、一次最適化のみを用いるアルゴリズムを導入することで、現状を改善しようと試みる。我々は、クリップされた確率比を用いた新しい目的関数を提案する。これは、方策のパフォーマンスの悲観的な推定（すなわち、下限）を形成する。方策を最適化するために、方策からのデータサンプリングと、サンプリングされたデータに対する複数エポックの最適化を交互に行う。

我々の実験では、代替目的関数の様々な異なるバージョンのパフォーマンスを比較し、クリップされた確率比を持つバージョンが最も優れていることを発見した。また、PPOを文献のいくつかの既存アルゴリズムと比較する。連続制御タスクでは、比較対象のアルゴリズムよりも優れたパフォーマンスを発揮する。Atariでは、A2Cよりも（サンプル効率の面で）大幅に優れており、ACERとほぼ同等のパフォーマンスを示すが、はるかに単純である。

¹DQNはArcade Learning Environment [Bel+15]のような離散行動空間を持つゲーム環境ではうまく機能するが、OpenAI Gym [Bro+16]やDuan et al. [Dua+16]によって記述された連続制御ベンチマークではうまく機能することが示されていない。

# 2 背景: 方策最適化

## 2.1 方策勾配法

方策勾配法は、方策勾配の推定量を計算し、それを確率的勾配上昇アルゴリズムに適用することで機能する。最も一般的に使用される勾配推定量は次の形式を持つ。

$$
\hat{g} = \hat{E}_t[\nabla_\theta \log \pi_\theta(a_t | S_t) \hat{A}_t]
$$

ここで、$\pi_\theta$は確率的方策、$A_t$はタイムステップ$t$におけるアドバンテージ関数の推定量である。$\hat{E}_t[\dots]$は、サンプリングと最適化を交互に行うアルゴリズムにおいて、有限バッチのサンプルに対する経験的平均を示す。自動微分ソフトウェアを使用する実装は、その勾配が方策勾配推定量となる目的関数を構築することで機能する。推定量$\hat{g}$は、目的関数を微分することによって得られる。

$$
L^{PG}(\theta) = \hat{E}_t[\log \pi_\theta(a_t | s_t)\hat{A}_t].
$$

この損失$L^{PG}$に対して同じ軌道を使用して複数回の最適化ステップを実行することは魅力的だが、それは十分に正当化されておらず、経験的にはしばしば破壊的に大きな方策更新につながる（セクション6.1を参照。結果は示されていないが、「クリッピングやペナルティなし」の設定と同様かそれよりも悪かった）。

## 2.2 信頼領域法

TRPO [Sch+15b]では、目的関数（「代替」目的関数）は、方策更新のサイズに関する制約の下で最大化される。具体的には、

$$
\underset{\theta}{\text{maximize}} \quad \hat{E}_t\left[\frac{\pi_\theta(a_t | S_t)}{\pi_{\theta_{\text{old}}}(a_t | S_t)}\hat{A}_t\right]
$$

$$
\text{subject to} \quad \hat{E}_t[\text{KL}[\pi_{\theta_{\text{old}}}(\cdot | S_t), \pi_\theta(\cdot | s_t)]] \le \delta.
$$

ここで、$\theta_{\text{old}}$は更新前の方策パラメータのベクトルである。この問題は、目的関数の線形近似と制約の二次近似を行った後、共役勾配アルゴリズムを使用して効率的に近似的に解くことができる。

TRPOを正当化する理論は、実際には制約の代わりにペナルティを使用すること、つまり、無制約の最適化問題を解くことを示唆する。

$$
\underset{\theta}{\text{maximize}} \quad \hat{E}_t\left[\frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | S_t)}\hat{A}_t - \beta \text{KL}[\pi_{\theta_{\text{old}}}(\cdot | S_t), \pi_\theta(\cdot | S_t)]\right]
$$

ここで$\beta$はいくつかの係数である。これは、特定のある代替目的関数（平均ではなく状態に対する最大KLを計算する）が方策$\pi$のパフォーマンスに対する下限（すなわち、悲観的境界）を形成するという事実から導かれる。TRPOは、異なる問題や、学習の過程で特性が変化する単一の問題においてさえ、うまく機能する単一の$\beta$値を選択することが難しいため、ペナルティではなくハード制約を使用する。したがって、TRPOの単調改善を模倣する一次アルゴリズムという我々の目標を達成するためには、固定されたペナルティ係数$\beta$を選択し、ペナルティ付き目的関数式(5)をSGDで最適化するだけでは不十分であり、追加の修正が必要であることが実験によって示されている。

# 3 クリッピングされた代替目的関数

確率比$r_t(\theta)$を$r_t(\theta) = \frac{\pi_\theta(a_t | S_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}$と定義する。したがって、$r(\theta_{\text{old}}) = 1$。TRPOは「代替」目的関数を最大化する。

$$
L^{CPI}(\theta) = \hat{E}_t\left[\frac{\pi_\theta(a_t | S_t)}{\pi_{\theta_{\text{old}}}(a_t | S_t)}\hat{A}_t\right] = \hat{E}_t[r_t(\theta)\hat{A}_t].
$$

上付き文字CPIは、この目的関数が提案された保守的な方策イテレーション[KL02]を指す。制約なしでは、$L^{CPI}$の最大化は過度に大きな方策更新につながる。そこで、今度は目的関数を変更して、$r_t(\theta)$が1から離れる方策の変更にペナルティを課す方法を検討する。

我々が提案する主要な目的関数は次の通りである。

$$
L^{CLIP}(\theta) = \hat{E}_t[\min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 – \epsilon, 1 + \epsilon)\hat{A}_t)]
$$

ここでイプシロンはハイパーパラメータであり、例えば$\epsilon = 0.2$とする。この目的関数の動機は次の通りである。`min`の中の第一項は$L^{CPI}$である。第二項である`clip(rt(θ), 1−ϵ, 1+ϵ)Ât`は、確率比をクリッピングすることで代替目的関数を修正し、確率比を区間$[1 - \epsilon, 1 + \epsilon]$の外に動かすインセンティブを取り除く。最後に、クリッピングされた目的とクリッピングされていない目的の最小値を取ることで、最終的な目的はクリッピングされていない目的の下限（すなわち、悲観的な下限）となる。このスキームでは、目的が改善する場合にのみ確率比の変化を無視し、目的が悪化する場合にはそれを含める。$L^{CLIP}(\theta) = L^{CPI}(\theta)$は$\theta_{\text{old}}$周りで一次的に等しい（つまり、$r=1$の場合）が、$\theta$が$\theta_{\text{old}}$から離れるにつれて異なってくることに注意。図1は、$L^{CLIP}$の単一項（つまり、単一の$t$）をプロットしたものである。アドバンテージが正または負であるかに応じて、確率比$r$が$1 - \epsilon$または$1 + \epsilon$でクリッピングされることに注意。

*図1: 確率比$r$の関数としての代替関数$L^{CLIP}$の単一項（単一のタイムステップ）のプロット。正のアドバンテージ（左）と負のアドバンテージ（右）の場合を示す。各プロットの赤い丸は最適化の開始点、すなわち$r=1$を示す。$L^{CLIP}$はこれらの項の多くを合計していることに注意。*

図2は、代替目的関数$L^{CLIP}$に関する別の直感を提供する。これは、PPO（後述のアルゴリズム）によって得られた方策更新方向に沿って補間する際に、いくつかの目的関数がどのように変化するかを示している。$L^{CLIP}$が$L^{CPI}$の下限であり、方策更新が大きすぎる場合にペナルティがあることがわかる。

*図2: 初期方策パラメータ$\theta_{\text{old}}$と、PPOの1回のイテレーション後に計算される更新された方策パラメータの間を補間する際の、代替目的関数。更新された方策は、初期方策から約0.02のKLダイバージェンスを持ち、この点が$L^{CLIP}$が最大となる点である。このプロットは、セクション6.1で提供されたハイパーパラメータを使用したHopper-v1問題に対する最初のポリシー更新に対応している。*

# 4 適応的KLペナルティ係数

クリップされた代替目的関数の代替として、またはそれに加えて使用できる別のアプローチは、KLダイバージェンスにペナルティを使用し、各方策更新でKLダイバージェンスの目標値$d_{\text{targ}}$を達成するようにペナルティ係数を調整することである。我々の実験では、KLペナルティはクリップされた代替目的関数よりもパフォーマンスが悪かったが、重要なベースラインであるため、ここに含めた。

このアルゴリズムの最も単純なインスタンス化では、各方策更新で以下のステップを実行する。

*   複数エポックのミニバッチSGDを使用して、KLペナルティ付き目的関数を最適化する。

    $$
    L^{KLPEN}(\theta) = \hat{E}_t\left[\frac{\pi_\theta(a_t | S_t)}{\pi_{\theta_{\text{old}}}(a_t | S_t)}\hat{A}_t - \beta \text{KL}[\pi_{\theta_{\text{old}}}(\cdot | S_t), \pi_\theta(\cdot | S_t)]\right]

*   $d = \hat{E}_t[\text{KL}[\pi_{\theta_{\text{old}}}(\cdot | S_t), \pi_\theta(\cdot | S_t)]]$を計算する。
    *   もし$d < d_{\text{targ}}/1.5$ならば、$\beta \leftarrow \beta/2$
    *   もし$d > d_{\text{targ}} \times 1.5$ならば、$\beta \leftarrow \beta \times 2$

更新された$\beta$は次のポリシー更新で使用される。このスキームでは、KLダイバージェンスが$d_{\text{targ}}$から大きく異なるポリシー更新が時折見られるが、これらはまれであり、$\beta$は迅速に調整される。上記のパラメータ1.5と2は経験的に選択されたものだが、アルゴリズムはそれらにあまり敏感ではない。$\beta$の初期値は別のハイパーパラメータだが、アルゴリズムが迅速に調整するため、実際には重要ではない。

# 5 アルゴリズム

前のセクションで述べた代替損失は、典型的な方策勾配実装にわずかな変更を加えるだけで計算および微分できる。自動微分ソフトウェアを使用する実装の場合、単に$L^{CLIP}$または$L^{KLPEN}$の損失を$L^{PG}$の代わりに構築し、この目的関数に対して確率的勾配上昇の複数ステップを実行する。

分散低減されたアドバンテージ関数推定量を計算するためのほとんどの技術は、学習された状態価値関数$V(s)$を利用する。例えば、一般化アドバンテージ推定[Sch+15a]、または[Mni+16]における有限期間推定器。方策と価値関数の間でパラメータを共有するニューラルネットワークアーキテクチャを使用する場合、方策代替と価値関数誤差項を組み合わせた損失関数を使用する必要がある。この目的関数は、十分な探索を保証するためにエントロピーボーナスを追加することでさらに拡張できる。これは、過去の研究[Wil92; Mni+16]で提案されている。これらの項を組み合わせると、各イテレーションで（近似的に）最大化される以下の目的関数が得られる。

$$
L^{CLIP+VF+S}(\theta) = \hat{E}_t[L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_\theta](S_t)],
$$

ここで、$c_1, c_2$は係数、$S$はエントロピーボーナス、$L^{VF}$は二乗誤差損失$(V_\theta(s_t) - V_{\text{targ}})^2$である。

[Mni+16]で普及し、再帰型ニューラルネットワークでの使用に適した方策勾配実装のスタイルでは、$T$タイムステップ（$T$はエピソード長よりもはるかに短い）にわたって方策を実行し、収集されたサンプルを更新に使用する。このスタイルは、タイムステップ$T$を超えて参照しないアドバンテージ推定器を必要とする。[Mni+16]で使用された推定器は次の通りである。

$$
\hat{A}_t = -V(s_t) + r_t + \gamma r_{t+1} + \dots + \gamma^{T-t+1} r_{T-1} + \gamma^{T-t} V(s_T)
$$

ここで$t$は長さ$T$の軌道セグメント内の$[0, T]$のタイムインデックスを示す。この選択肢を一般化すると、$\lambda = 1$の場合に式(10)に還元される、打ち切り版の一般化アドバンテージ推定を使用できる。

$$
\hat{A}_t = \delta_t + (\gamma\lambda)\delta_{t+1} + \dots + (\gamma\lambda)^{T-t+1}\delta_{T-1},
$$

ここで$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$

固定長軌道セグメントを使用する近傍方策最適化（PPO）アルゴリズムを以下に示す。各イテレーションで、$N$個の（並列）アクターが$T$タイムステップのデータを収集する。次に、これらの$NT$タイムステップのデータに基づいて代替損失を構築し、$K$エポックの間ミニバッチSGD（通常はより良いパフォーマンスのためにAdam [KB14]）で最適化する。

**アルゴリズム1 PPO、アクター・クリティックスタイル**
- for iteration=1,2,... do
  - for actor=1, 2, ..., N do
    - 方策 $\pi_{\theta_{\text{old}}}$ を環境内で T タイムステップ実行
    - アドバンテージ推定値 $\hat{A}_1,...,\hat{A}_T$ を計算
  - end for
  - 代替損失 $L$ を $\theta$ について、$K$ エポックおよびミニバッチサイズ $M < NT$ で最適化
  - $\theta_{\text{old}} \leftarrow \theta$
- end for

# 6 実験

## 6.1 代替目的関数の比較

まず、いくつかの異なる代替目的関数を異なるハイパーパラメータの下で比較する。ここでは、$L^{CLIP}$代替目的関数をいくつかの自然なバリエーションおよび削除されたバージョンと比較する。

クリッピングまたはペナルティなし:
$L_t(\theta) = r_t(\theta) \hat{A}_t$

クリッピング:
$L_t(\theta) = \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta)), 1 – \epsilon, 1 + \epsilon)\hat{A}_t$

KLペナルティ（固定または適応的）:
$L_t(\theta) = r_t(\theta)\hat{A}_t – \beta \text{KL}[\pi_{\theta_{\text{old}}}, \pi_\theta]$

KLペナルティについては、セクション4で説明されているように、固定のペナルティ係数$\beta$またはターゲットKL値$d_{\text{targ}}$を使用する適応係数のいずれかを使用できる。ログ空間でのクリッピングも試したが、パフォーマンスは向上しなかった。

各アルゴリズムバリアントのハイパーパラメータを探索しているため、計算コストの低いベンチマークでアルゴリズムをテストした。具体的には、OpenAI Gym [Bro+16]に実装された7つのシミュレートされたロボティクス ${}^2$ タスクを使用し、MuJoCo [TET12]物理エンジンを使用する。各タスクで100万タイムステップの学習を行った。探索対象のハイパーパラメータ（$\epsilon$、$\beta$、$d_{\text{targ}}$）以外については、表3に記載されている。

方策を表現するために、2つの隠れ層と64ユニット、tanh非線形性を持ち、[Sch+15b; Dua+16]に従って可変標準偏差を持つガウス分布の平均を出力する全結合MLPを使用した。方策と価値関数の間でパラメータを共有しない（そのため係数$c_1$は無関係）、またエントロピーボーナスは使用しない。

各アルゴリズムは、7つの環境すべてで3つのランダムシードで実行された。各アルゴリズムの実行は、最後の100エピソードの平均総報酬を計算して評価した。各環境のスコアは、ランダム方策がスコア0を、最高のランダム方策がスコア1を与えるようにシフトおよびスケーリングし、21回の実行を平均して各アルゴリズム設定に対する単一のスカラーを生成した。

結果は表1に示されている。クリッピングやペナルティなしの設定ではスコアが負になっていることに注意。これは、1つの環境（ハーフチーター）で非常に負のスコアになり、初期のランダム方策よりも悪いためである。

| algorithm | avg. normalized score |
|:---|:---:|
| No clipping or penalty | -0.39 |
| Clipping, $\epsilon = 0.1$ | 0.76 |
| Clipping, $\epsilon = 0.2$ | **0.82** |
| Clipping, $\epsilon = 0.3$ | 0.70 |
| Adaptive KL $d_{\text{targ}} = 0.003$ | 0.68 |
| Adaptive KL $d_{\text{targ}} = 0.01$ | 0.74 |
| Adaptive KL $d_{\text{targ}} = 0.03$ | 0.71 |
| Fixed KL, $\beta = 0.3$ | 0.62 |
| Fixed KL, $\beta = 1.$ | 0.71 |
| Fixed KL, $\beta = 3.$ | 0.72 |
| Fixed KL, $\beta = 10.$ | 0.69 |

*表1: 連続制御ベンチマークの結果。各アルゴリズム/ハイパーパラメータ設定に対する正規化された平均スコア（21回の実行、7つの環境）。$\beta$は1で初期化された。*

## 6.2 連続領域における他のアルゴリズムとの比較

次に、PPO（セクション3の「クリップされた」代替目的関数を使用）を、連続問題に有効と見なされている文献の他のいくつかの手法と比較する。我々は、以下のアルゴリズムの調整された実装と比較した: 信頼領域方策最適化 [Sch+15b]、交差エントロピー法（CEM）[SL06]、適応ステップサイズ付きバニラ方策勾配 ${}^3$、

${}^2$HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, and Walker2d, すべて"-v1"
${}^3$各データバッチの後、Adamステップサイズは、セクション4に示されているものと同様のルールを使用して、元のポリシーと更新されたポリシーのKLダイバージェンスに基づいて調整される。実装は [https://github.com/berkeleydeeprlcourse/homework/tree/master/hw4](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw4) で入手可能。

A2C [Mni+16]、A2C（信頼領域付き）[Wan+16]。A2Cはアドバンテージアクタークリティックの略で、A3Cの同期バージョンである。A3Cは非同期バージョンと同等かそれ以上のパフォーマンスを持つことがわかった。PPOには、前セクションのハイパーパラメータを使用し、$\epsilon = 0.2$とした。PPOは、ほとんどすべての連続制御環境で既存の手法を上回る結果を示している。

*図3: MuJoCoのいくつかの環境における、100万タイムステップの学習に対する複数のアルゴリズムの比較。*

## 6.3 連続領域での事例紹介: 人型ロボットの走行と操縦

高次元の連続制御問題におけるPPOのパフォーマンスを示すため、3Dヒューマノイドが走行、操縦、そしてキューブによって叩かれながら地面から立ち上がる必要がある一連の問題で学習を行った。テストした3つのタスクは次の通りである。(1) RoboschoolHumanoid: 前方移動のみ、(2) RoboschoolHumanoidFlagrun: ターゲットの位置が200タイムステップごとにランダムに変化するか、目標に到達すると変化する、(3) RoboschoolHumanoid-FlagrunHarder: キューブによってロボットが叩かれ、地面から立ち上がる必要がある。学習された方策の静止画像は図5、3つのタスクの学習曲線は図4を参照。ハイパーパラメータは表4で提供されている。関連研究として、Heess et al. [Hee+17] は、3Dロボットの移動方策を学習するために、PPOの適応的KLバリアント（セクション4）を使用した。

*図4: Roboschoolを使用した3Dヒューマノイド制御タスクにおけるPPOの学習曲線。*

*図5: RoboschoolHumanoidFlagrunから学習された方策の静止フレーム。最初の6フレームでは、ロボットがターゲットに向かって走る。その後、位置がランダムに変化し、ロボットは新しいターゲットに向かって向きを変えて走る。*

## 6.4 Atari領域における他のアルゴリズムとの比較

また、PPOをArcade Learning Environment [Bel+15]ベンチマークで実行し、調整済みのA2C [Mni+16]とACER [Wan+16]の実装と比較した。3つのアルゴリズムすべてで、[Mni+16]で使用されたものと同じ方策ネットワークアーキテクチャを使用した。PPOのハイパーパラメータは表5で提供されている。他の2つのアルゴリズムについては、このベンチマークでのパフォーマンスを最大化するために調整されたハイパーパラメータを使用した。

すべての49ゲームの結果と学習曲線は付録Bで提供されている。以下の2つのスコアリングメトリクスを検討する。(1) 全学習期間におけるエピソードごとの平均報酬（高速学習を重視）、および (2) 学習の最後の100エピソードにおけるエピソードごとの平均報酬（最終パフォーマンスを重視）。表2は、各アルゴリズムが「勝利」したゲームの数を示す。勝者は、3つの試行にわたるスコアリングメトリクスを平均して計算される。

| | A2C | ACER | PPO | Tie |
|:---|:---:|:---:|:---:|:---:|
| (1) 学習期間全体の平均エピソード報酬 | 1 | 18 | **30** | 0 |
| (2) 最後の100エピソードの平均エピソード報酬 | 1 | **28** | 19 | 1 |

*表2: 各アルゴリズムが「勝利」したゲームの数。スコアリングメトリクスは3つの試行で平均化されている。*

# 7 結論

我々は、各方策更新を実行するために確率的勾配上昇の複数エポックを使用する、近傍方策最適化という方策最適化手法のファミリーを導入した。これらの手法は、信頼領域手法の安定性と信頼性を持つが、実装ははるかに単純であり、バニラ方策勾配実装へのコード変更はわずかで済み、より一般的な設定（例えば、方策と価値関数のための結合アーキテクチャを使用する場合）にも適用可能であり、全体的に優れたパフォーマンスを発揮する。

# 8 謝辞

Rocky Duan、Peter Chen、およびOpenAIの他の人々からの洞察に富んだコメントに感謝します。

# 参考文献

[Bel+15] M. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. "The arcade learning environment: An evaluation platform for general agents". In: *Twenty-Fourth International Joint Conference on Artificial Intelligence*. 2015.

[Bro+16] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. “OpenAI Gym”. In: *arXiv preprint arXiv:1606.01540* (2016).

[Dua+16] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. “Benchmarking Deep Reinforcement Learning for Continuous Control". In: *arXiv preprint arXiv:1604.06778* (2016).

[Hee+17] N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, A. Eslami, M. Riedmiller, et al. "Emergence of Locomotion Behaviours in Rich Environments". In: *arXiv preprint arXiv:1707.02286* (2017).

[KL02] S. Kakade and J. Langford. “Approximately optimal approximate reinforcement learning". In: *ICML*. Vol. 2. 2002, pp. 267-274.

[KB14] D. Kingma and J. Ba. “Adam: A method for stochastic optimization". In: *arXiv preprint arXiv:1412.6980* (2014).

[Mni+15] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. "Human-level control through deep reinforcement learning". In: *Nature 518.7540* (2015), pp. 529-533.

[Mni+16] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. "Asynchronous methods for deep reinforcement learning". In: *arXiv preprint arXiv:1602.01783* (2016).

[Sch+15a] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. “High-dimensional continuous control using generalized advantage estimation". In: *arXiv preprint arXiv:1506.02438* (2015).

[Sch+15b] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. “Trust region policy optimization". In: *CoRR, abs/1502.05477* (2015).

[SL06] I. Szita and A. Lörincz. “Learning Tetris using the noisy cross-entropy method". In: *Neural computation 18.12* (2006), pp. 2936–2941.

[TET12] E. Todorov, T. Erez, and Y. Tassa. “MuJoCo: A physics engine for model-based control”. In: *Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on*. IEEE. 2012, pp. 5026-5033.

[Wan+16] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. "Sample Efficient Actor-Critic with Experience Replay". In: *arXiv preprint arXiv:1611.01224* (2016).

[Wil92] R. J. Williams. “Simple statistical gradient-following algorithms for connectionist reinforcement learning". In: *Machine learning 8.3-4* (1992), pp. 229-256.

## A ハイパーパラメータ

| Hyperparameter | Value |
|:---|:---|
| Horizon (T) | 2048 |
| Adam stepsize | $3 \times 10^{-4}$ |
| Num. epochs | 10 |
| Minibatch size | 64 |
| Discount ($\gamma$) | 0.99 |
| GAE parameter ($\lambda$) | 0.95 |

*表3: MuJoCo 100万タイムステップベンチマークで使用されたPPOハイパーパラメータ。*

| Hyperparameter | Value |
|:---|:---|
| Horizon (T) | 512 |
| Adam stepsize | -- |
| Num. epochs | 15 |
| Minibatch size | 4096 |
| Discount ($\gamma$) | 0.99 |
| GAE parameter ($\lambda$) | 0.95 |
| Number of actors | 32 (locomotion), 128 (flagrun) |
| Log stdev. of action distribution | LinearAnneal($-0.7, -1.6$) |

*表4: Roboschool実験で使用されたPPOハイパーパラメータ。AdamのステップサイズはKLダイバージェンスの目標値に基づいて調整された。*

| Hyperparameter | Value |
|:---|:---|
| Horizon (T) | 128 |
| Adam stepsize | $2.5 \times 10^{-4} \times \alpha$ |
| Num. epochs | 3 |
| Minibatch size | $32 \times 8$ |
| Discount ($\gamma$) | 0.99 |
| GAE parameter ($\lambda$) | 0.95 |
| Number of actors | 8 |
| Clipping parameter $\epsilon$ | $0.1 \times \alpha$ |
| VF coeff. $c_1$ (9) | 1 |
| Entropy coeff. $c_2$ (9) | 0.01 |

*表5: Atari実験で使用されたPPOハイパーパラメータ。$\alpha$は1から0へ線形にアニールされる。*

## B その他のAtariゲームでのパフォーマンス

ここでは、PPOとA2Cのより大規模な49のAtariゲームコレクションでの比較を示す。図6は3つのランダムシードでの学習曲線を示し、表6は平均パフォーマンスを示す。

*図6: 公開時にOpenAI Gymに含まれていた49のATARIゲームすべてにおけるPPOとA2Cの比較。*

| | A2C | ACER | PPO |
|:---|:---:|:---:|:---:|
| Alien | 1141.7 | 1655.4 | **1850.3** |
| Amidar | 380.8 | **827.6** | 674.6 |
| Assault | 1562.9 | 4653.8 | **4971.9** |
| Asterix | 3176.3 | **6801.2** | 4532.5 |
| Asteroids | 1653.3 | **2389.3** | 2097.5 |
| Atlantis | 729265.3 | 1841376.0 | **2311815.0** |
| BankHeist | 1095.3 | 1177.5 | **1280.6** |
| BattleZone | 3080.0 | 8983.3 | **17366.7** |
| BeamRider | 3031.7 | **3863.3** | 1590.0 |
| Bowling | 30.1 | 33.3 | **40.1** |
| Boxing | 17.7 | **98.9** | 94.6 |
| Breakout | 303.0 | **456.4** | 274.8 |
| Centipede | 3496.5 | **8904.8** | 4386.4 |
| ChopperCommand | 1171.7 | **5287.7** | 3516.3 |
| CrazyClimber | 107770.0 | **132461.0** | 110202.0 |
| DemonAttack | 6639.1 | **38808.3** | 11378.4 |
| DoubleDunk | -16.2 | **-13.2** | -14.9 |
| Enduro | 0.0 | 0.0 | **758.3** |
| FishingDerby | 20.6 | **34.7** | 17.8 |
| Freeway | 0.0 | 0.0 | **32.5** |
| Frostbite | 261.8 | 285.6 | **314.2** |
| Gopher | 1500.9 | **37802.3** | 2932.9 |
| Gravitar | 194.0 | 225.3 | **737.2** |
| IceHockey | -6.4 | -5.9 | **-4.2** |
| Jamesbond | 52.3 | 261.8 | **560.7** |
| Kangaroo | 45.3 | 50.0 | **9928.7** |
| Krull | **8367.4** | 7268.4 | 7942.3 |
| KungFuMaster | 24900.3 | **27599.3** | 23310.3 |
| MontezumaRevenge | 0.0 | 0.3 | **42.0** |
| MsPacman | 1626.9 | **2718.5** | 2096.5 |
| NameThisGame | 5961.2 | **8488.0** | 6254.9 |
| Pitfall | -55.0 | **-16.9** | -32.9 |
| Pong | 19.7 | **20.7** | 20.7 |
| PrivateEye | 91.3 | **182.0** | 69.5 |
| Qbert | 10065.7 | **15316.6** | 14293.3 |
| Riverraid | 7653.5 | **9125.1** | 8393.6 |
| RoadRunner | 32810.0 | **35466.0** | 25076.0 |
| Robotank | 2.2 | 2.5 | **5.5** |
| Seaquest | 1714.3 | **1739.5** | 1204.5 |
| SpaceInvaders | 744.5 | **1213.9** | 942.5 |
| StarGunner | 26204.0 | **49817.7** | 32689.0 |
| Tennis | -22.2 | -17.6 | **-14.8** |
| TimePilot | 2898.0 | 4175.7 | **4342.0** |
| Tutankham | 206.8 | **280.8** | 254.4 |
| UpNDown | 17369.8 | **145051.4** | 95445.0 |
| Venture | 0.0 | 0.0 | 0.0 |
| VideoPinball | 19735.9 | **156225.6** | 37389.0 |
| WizardOfWor | 859.0 | 2308.3 | **4185.3** |
| Zaxxon | 16.3 | 29.0 | **5008.7** |

*表6: AtariゲームにおけるPPOとA2Cの平均最終スコア（最後の100エピソード、40Mゲームフレーム（10Mタイムステップ）後）。*
